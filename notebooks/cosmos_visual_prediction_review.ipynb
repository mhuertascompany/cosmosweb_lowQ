{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe1429b",
   "metadata": {},
   "source": [
    "# COSMOS Visual Zoobot Evaluation\n",
    "\n",
    "Use this notebook to inspect the `test_set.csv` predictions (class probabilities per galaxy) and compare them against ground-truth labels to build confusion matrices, per-class statistics, and other quick diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d6e3e",
   "metadata": {},
   "source": [
    "## 1. Configure file locations\n",
    "\n",
    "* `PREDICTIONS_CSV` points to the exported probability table (e.g. the `test_set.csv` downloaded from the cluster).\n",
    "* Provide ground-truth labels either by (a) pointing `GROUND_TRUTH_CSV` to a saved test catalog that contains `id_str` and `label`, or (b) setting `REBUILD_GROUND_TRUTH=True` and supplying the same catalog/DB paths that were used during training so the notebook can recreate the splits locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd65817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Required ---\n",
    "PREDICTIONS_CSV = Path(\"/Users/marchuertascompany/Documents/data/COSMOS-Web/zoobot/ilbert/test_set.csv\")\n",
    "\n",
    "# Option A: load a saved catalog with true labels (id_str, label[, label_name,...])\n",
    "GROUND_TRUTH_CSV = None  # Path(\"/Users/.../test_catalog.csv\")\n",
    "\n",
    "# Option B: rebuild the catalog locally (set to True and fill the paths below)\n",
    "REBUILD_GROUND_TRUTH = False\n",
    "STAMP_DIR = Path(\"/n03data/huertas/COSMOS-Web/zoobot/stamps/f150w\")\n",
    "VISUAL_LABELS = Path(\"/n07data/ilbert/COSMOS-Web/photoz_MASTER_v3.1.0/MORPHO/visualmorpho_COSMOSWeb_v7.db\")\n",
    "SQLITE_TABLE = \"morphology\"\n",
    "FILTER_NAME = \"F150W\"\n",
    "FILENAME_TEMPLATE = \"{filter}_{id}.jpg\"\n",
    "KEEP_AMBIGUOUS = False\n",
    "MAX_GALAXIES = None\n",
    "TEST_FRACTION = 0.2\n",
    "VAL_FRACTION = 0.1\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad31232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from moprhology.zoobot import train_on_cosmos_visual as cosmos\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45689349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_test_catalog():\n",
    "    df_visual = cosmos.load_visual_catalog(VISUAL_LABELS, SQLITE_TABLE)\n",
    "    catalog = cosmos.attach_stamps(\n",
    "        df_visual=df_visual,\n",
    "        stamp_dir=STAMP_DIR,\n",
    "        filename_template=FILENAME_TEMPLATE,\n",
    "        filter_name=FILTER_NAME,\n",
    "        keep_ambiguous=KEEP_AMBIGUOUS\n",
    "    )\n",
    "    catalog = cosmos.maybe_subsample(catalog, MAX_GALAXIES, SEED)\n",
    "    train_catalog, val_catalog, test_catalog = cosmos.stratified_splits(\n",
    "        catalog,\n",
    "        test_fraction=TEST_FRACTION,\n",
    "        val_fraction=VAL_FRACTION,\n",
    "        seed=SEED\n",
    "    )\n",
    "    return test_catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(PREDICTIONS_CSV)\n",
    "prob_cols = [col for col in predictions.columns if col.startswith('p_')]\n",
    "if not prob_cols:\n",
    "    raise ValueError(\"No probability columns (prefixed with 'p_') found in predictions file.\")\n",
    "\n",
    "truth = None\n",
    "if GROUND_TRUTH_CSV is not None:\n",
    "    truth = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "elif REBUILD_GROUND_TRUTH:\n",
    "    truth = rebuild_test_catalog()\n",
    "\n",
    "if truth is not None and 'id_str' not in truth.columns:\n",
    "    raise ValueError(\"Ground-truth catalog must contain an 'id_str' column.\")\n",
    "\n",
    "predictions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a53fc",
   "metadata": {},
   "source": [
    "## 2. Attach class predictions\n",
    "\n",
    "Derive the most probable class for each galaxy and the associated confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = cosmos.CLASS_COLUMNS\n",
    "if len(prob_cols) != len(label_names):\n",
    "    print(\"Warning: probability column count does not match CLASS_COLUMNS length.\")\n",
    "\n",
    "prob_matrix = predictions[prob_cols].to_numpy()\n",
    "pred_indices = prob_matrix.argmax(axis=1)\n",
    "pred_labels = [label_names[i] for i in pred_indices]\n",
    "pred_confidence = prob_matrix.max(axis=1)\n",
    "\n",
    "predictions['pred_label'] = pred_labels\n",
    "predictions['pred_idx'] = pred_indices\n",
    "predictions['pred_confidence'] = pred_confidence\n",
    "\n",
    "predictions[['id_str', 'pred_label', 'pred_confidence'] + prob_cols].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd48b10",
   "metadata": {},
   "source": [
    "## 3. Merge with ground truth (if available)\n",
    "\n",
    "Metrics are computed only when true labels are provided. Set `GROUND_TRUTH_CSV` or enable the rebuild block above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a56eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truth is None:\n",
    "    print(\"Ground-truth labels not available. Provide GROUND_TRUTH_CSV or enable REBUILD_GROUND_TRUTH to continue with metrics.\")\n",
    "else:\n",
    "    truth_cols = ['id_str']\n",
    "    if 'label' in truth.columns:\n",
    "        truth_cols.append('label')\n",
    "    if 'true_label' in truth.columns and 'true_label' not in truth_cols:\n",
    "        truth_cols.append('true_label')\n",
    "    merged = predictions.merge(truth[truth_cols], on='id_str', how='inner', suffixes=('', '_true'))\n",
    "    label_to_index = {name: idx for idx, name in enumerate(cosmos.CLASS_COLUMNS)}\n",
    "    if 'label' in merged.columns:\n",
    "        if merged['label'].dtype == object and merged['label'].isin(label_to_index.keys()).all():\n",
    "            merged['true_label'] = merged['label']\n",
    "            merged['true_idx'] = merged['true_label'].map(label_to_index)\n",
    "        else:\n",
    "            merged['true_idx'] = merged['label'].astype(int)\n",
    "            merged['true_label'] = merged['true_idx'].map(lambda idx: cosmos.CLASS_COLUMNS[idx])\n",
    "    elif 'true_label' in merged.columns:\n",
    "        merged['true_idx'] = merged['true_label'].map(label_to_index)\n",
    "    else:\n",
    "        raise ValueError(\"Ground-truth data must contain a 'label' (int) or 'true_label' (string) column.\")\n",
    "\n",
    "    print(f\"Merged {len(merged)} rows with ground truth out of {len(predictions)} predictions.\")\n",
    "    merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e9355",
   "metadata": {},
   "source": [
    "## 4. Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truth is None:\n",
    "    print(\"Skipping metrics because ground truth is missing.\")\n",
    "else:\n",
    "    acc = accuracy_score(merged['true_idx'], merged['pred_idx'])\n",
    "    print(f\"Overall accuracy: {acc:.4f}\")\n",
    "    report = classification_report(\n",
    "        merged['true_idx'],\n",
    "        merged['pred_idx'],\n",
    "        target_names=cosmos.CLASS_COLUMNS,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d2cd9",
   "metadata": {},
   "source": [
    "## 5. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truth is None:\n",
    "    print(\"Confusion matrix unavailable without ground truth.\")\n",
    "else:\n",
    "    cm = confusion_matrix(\n",
    "        merged['true_idx'],\n",
    "        merged['pred_idx'],\n",
    "        labels=list(range(len(cosmos.CLASS_COLUMNS)))\n",
    "    )\n",
    "    cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax[0], xticklabels=cosmos.CLASS_COLUMNS, yticklabels=cosmos.CLASS_COLUMNS)\n",
    "    ax[0].set_title('Confusion matrix (counts)')\n",
    "    ax[0].set_xlabel('Predicted')\n",
    "    ax[0].set_ylabel('True')\n",
    "\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=ax[1], xticklabels=cosmos.CLASS_COLUMNS, yticklabels=cosmos.CLASS_COLUMNS)\n",
    "    ax[1].set_title('Confusion matrix (row-normalized)')\n",
    "    ax[1].set_xlabel('Predicted')\n",
    "    ax[1].set_ylabel('True')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb757e",
   "metadata": {},
   "source": [
    "## 6. Per-class confidence summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8cac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truth is None:\n",
    "    print(\"Provide ground-truth labels to compute per-class summaries.\")\n",
    "else:\n",
    "    per_class = (\n",
    "        merged.groupby('true_label')\n",
    "        .agg(\n",
    "            support=('true_label', 'size'),\n",
    "            accuracy=('pred_idx', lambda idx: np.mean(idx == merged.loc[idx.index, 'true_idx'])),\n",
    "            mean_confidence=('pred_confidence', 'mean'),\n",
    "            median_confidence=('pred_confidence', 'median')\n",
    "        )\n",
    "        .sort_values('support', ascending=False)\n",
    "    )\n",
    "    display(per_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6d83a",
   "metadata": {},
   "source": [
    "## 7. Inspect largest errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truth is None:\n",
    "    print(\"Ground truth required to list misclassifications.\")\n",
    "else:\n",
    "    prob_array = merged[prob_cols].to_numpy()\n",
    "    true_probs = prob_array[np.arange(len(merged)), merged['true_idx']]\n",
    "    errors = merged[merged['pred_label'] != merged['true_label']].copy()\n",
    "    errors['true_prob'] = true_probs[errors.index]\n",
    "    errors['confidence_gap'] = errors['pred_confidence'] - errors['true_prob']\n",
    "    display(errors.sort_values('pred_confidence', ascending=False).head(20)[['id_str', 'true_label', 'pred_label', 'pred_confidence', 'true_prob', 'confidence_gap']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
