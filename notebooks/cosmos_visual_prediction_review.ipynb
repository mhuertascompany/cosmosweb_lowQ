{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe1429b",
   "metadata": {},
   "source": [
    "# COSMOS Visual Zoobot Evaluation\n",
    "\n",
    "This notebook compares Zoobot predictions against the saved COSMOS-Web visual labels. Place both `test_set_*.csv` (model probabilities) and `test_catalog.csv` (ground-truth labels) on your local machine and set their paths below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d6e3e",
   "metadata": {},
   "source": [
    "## 1. Configure file locations\n",
    "\n",
    "Set the local paths to the exported prediction CSV (`test_set_*.csv`) and the `test_catalog.csv` containing the ground-truth labels. No remote catalogs or stamps are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd65817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PREDICTIONS_CSV = Path(\"/Users/marchuertascompany/Documents/data/COSMOS-Web/zoobot/ilbert/test_set_nano.csv\")\n",
    "GROUND_TRUTH_CSV = Path(\"/Users/marchuertascompany/Documents/data/COSMOS-Web/zoobot/ilbert/test_catalog.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad31232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path(\"/Users/marchuertascompany/Documents/python_scripts/cosmosweb_lowQ\")\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from moprhology.zoobot import train_on_cosmos_visual as cosmos\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(PREDICTIONS_CSV)\n",
    "prob_cols = [col for col in predictions.columns if col.startswith('p_')]\n",
    "if not prob_cols:\n",
    "    raise ValueError(\"No probability columns (prefixed with 'p_') found in predictions file.\")\n",
    "\n",
    "truth = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "if 'id_str' not in truth.columns:\n",
    "    raise ValueError(\"Ground-truth catalog must contain an 'id_str' column.\")\n",
    "\n",
    "predictions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a53fc",
   "metadata": {},
   "source": [
    "## 2. Attach class predictions\n",
    "\n",
    "Derive the most probable class for each galaxy and the associated confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [col[2:] for col in prob_cols]\n",
    "prob_matrix = predictions[prob_cols].to_numpy()\n",
    "pred_indices = prob_matrix.argmax(axis=1)\n",
    "pred_labels = [label_names[i] for i in pred_indices]\n",
    "pred_confidence = prob_matrix.max(axis=1)\n",
    "\n",
    "predictions['pred_label'] = pred_labels\n",
    "predictions['pred_idx'] = pred_indices\n",
    "predictions['pred_confidence'] = pred_confidence\n",
    "\n",
    "predictions[['id_str', 'pred_label', 'pred_confidence'] + prob_cols].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd48b10",
   "metadata": {},
   "source": [
    "## 3. Merge with ground truth (if available)\n",
    "\n",
    "Metrics are computed only when true labels are provided. Set `GROUND_TRUTH_CSV` or enable the rebuild block above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a56eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_cols = ['id_str', 'label'] if 'label' in truth.columns else ['id_str', 'true_label']\n",
    "merged = predictions.merge(truth[truth_cols], on='id_str', how='inner', suffixes=('', '_true'))\n",
    "label_to_index = {name: idx for idx, name in enumerate(label_names)}\n",
    "\n",
    "if 'label' in merged.columns:\n",
    "    if merged['label'].dtype == object and merged['label'].isin(label_to_index.keys()).all():\n",
    "        merged['true_label'] = merged['label']\n",
    "        merged['true_idx'] = merged['true_label'].map(label_to_index)\n",
    "    else:\n",
    "        merged['true_idx'] = merged['label'].astype(int)\n",
    "        merged['true_label'] = merged['true_idx'].map(lambda idx: label_names[idx])\n",
    "elif 'true_label' in merged.columns:\n",
    "    merged['true_idx'] = merged['true_label'].map(label_to_index)\n",
    "else:\n",
    "    raise ValueError(\"Ground-truth data must contain a 'label' (int) or 'true_label' (string) column.\")\n",
    "\n",
    "print(f\"Merged {len(merged)} rows with ground truth out of {len(predictions)} predictions.\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e9355",
   "metadata": {},
   "source": [
    "## 4. Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'merged' not in globals():\n",
    "    raise RuntimeError(\"Run the merge cell above before computing metrics.\")\n",
    "\n",
    "acc = accuracy_score(merged['true_idx'], merged['pred_idx'])\n",
    "print(f\"Overall accuracy: {acc:.4f}\")\n",
    "report = classification_report(\n",
    "    merged['true_idx'],\n",
    "    merged['pred_idx'],\n",
    "    target_names=label_names,\n",
    "    zero_division=0\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d2cd9",
   "metadata": {},
   "source": [
    "## 5. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'merged' not in globals():\n",
    "    raise RuntimeError(\"Run the merge cell above before plotting the confusion matrix.\")\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    merged['true_idx'],\n",
    "    merged['pred_idx'],\n",
    "    labels=list(range(len(label_names)))\n",
    ")\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax[0], xticklabels=label_names, yticklabels=label_names)\n",
    "ax[0].set_title('Confusion matrix (counts)')\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=ax[1], xticklabels=label_names, yticklabels=label_names)\n",
    "ax[1].set_title('Confusion matrix (row-normalized)')\n",
    "ax[1].set_xlabel('Predicted')\n",
    "ax[1].set_ylabel('True')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb757e",
   "metadata": {},
   "source": [
    "## 6. Per-class confidence summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8cac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class = (\n",
    "    merged.groupby('true_label')\n",
    "    .agg(\n",
    "        support=('true_label', 'size'),\n",
    "        accuracy=('pred_idx', lambda idx: np.mean(idx == merged.loc[idx.index, 'true_idx'])),\n",
    "        mean_confidence=('pred_confidence', 'mean'),\n",
    "        median_confidence=('pred_confidence', 'median')\n",
    "    )\n",
    "    .sort_values('support', ascending=False)\n",
    ")\n",
    "display(per_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6d83a",
   "metadata": {},
   "source": [
    "## 7. Inspect largest errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array = merged[prob_cols].to_numpy()\n",
    "true_probs = prob_array[np.arange(len(merged)), merged['true_idx']]\n",
    "errors = merged[merged['pred_label'] != merged['true_label']].copy()\n",
    "errors['true_prob'] = true_probs[errors.index]\n",
    "errors['confidence_gap'] = errors['pred_confidence'] - errors['true_prob']\n",
    "display(errors.sort_values('pred_confidence', ascending=False).head(20)[\n",
    "    ['id_str', 'true_label', 'pred_label', 'pred_confidence', 'true_prob', 'confidence_gap']\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}